# -*- coding: utf-8 -*-
"""Rich vs Poor dataset Classification

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ly3sgG9l9-ldeCCN-N53C4DaqLbDekuQ

#***Project Title : Rich VS Poor***
---

##***1.Description***
---
* **Purpose:** Predict whether a person is Rich or Poor.

* **Problem Type:** Binary Classification (Target → Rich / Poor).

* **Size:** 22,792 rows × 16 columns.

* **Target Column:** Predictions (Rich / Poor).

###**Main Features:**

* age – Age of the individual.

* Working Section – Employment type (Private, Govt, Self-employed, etc.).

* Financial Weight – Economic weight/score.

* education & education-num – Highest education level and its numeric form.

* Marriage Status – Marital condition (Married, Divorced, etc.).

* occupation – Type of job.

* Relationship Status – Role in household (Husband, Wife, Child, etc.).

* Skin Color – Ethnic/race category.

* Gender – Male/Female.

* Capital Gain / Capital Loss – Additional financial indicators.

* hours-per-week – Weekly working hours.

* Country – Native country.

* id – Unique identifier.

---

##***2.Important Libraries***
---
*Libraries are pre-written collections of functions and tools that simplify programming tasks.*

####**Insights**:

* pandas, numpy → Data manipulation & analysis.

* matplotlib, seaborn → Data visualization.

* sklearn, imblearn → Machine Learning & preprocessing.

* warnings → Suppress unnecessary alerts.
---
"""

# used to avoid warnigs
import warnings
warnings.filterwarnings('ignore')

# used for data manuplation and analysis
import pandas as pd
import numpy as np

# used for data visualization
import matplotlib.pyplot as plt
import seaborn as sns

#used for perform veriours functions to perform Classification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Classification Models
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier

#Evulation function
from sklearn.metrics import confusion_matrix
from sklearn.metrics import  classification_report
from sklearn import metrics

"""##***3.Load and Read dataset***
---
*Loading dataset means importing raw data (CSV file) into Python for analysis.*

####**Insights:**

* pd.read_csv() loads the dataset into a DataFrame.

* Shape = 22792 rows × 16 columns.

* Columns include demographics (age, gender, Skin Color) and Predictions(Target) etc

---
"""

#Loaad Dataset
df = pd.read_csv('/content/richvspoor.zip')
# Read dataset
df

"""##***4.Data Exploration***
---
*Exploration is the first step of EDA to understand dataset size, structure, and features.*

####**Insights:**

* df.head() → previews first rows.

* df.info() → shows column types & non-null values.

* df.shape & columns → total row and columns & columns name

* df.describe() → summary stats for numerical columns.

* df.dtypes → all columnms data types

* value_counts() → check value counts of categorical columns

* Seprate integer , float and categorical columns
---
"""

# show first 5 rows
df.head()

# Show total rows and columns
print("Rows and columns: ",df.shape)
# Show all columns
print("\nAll Columns:",df.columns)

# Information about dataset
df.info()

# summary stats for numerical columns.
df.describe()

# data type of all columns
df.dtypes

# seprate integer , Float and categorical columns
cat_cols = df.select_dtypes(include= ['object']).columns.tolist()
num_cols = df.select_dtypes(include= ['int64']).columns.tolist()
float_cols = df.select_dtypes(include= ['float64']).columns.tolist()

print("Categorical columns:",cat_cols)
print("\nNumerical columns:",num_cols)
print("\nFloat columns:",float_cols)

cat_cols = ['Working Section', 'education', 'Marriage Status', 'occupation', 'Relationship Status', 'Skin Color', 'Gender', 'Country', 'Predictions']
#Value colums of Categorical columns
for i in cat_cols:
  print(df[i].value_counts())

"""##***5.Data Cleaning***
---
*Cleaning prepares raw data by removing errors, duplicates, and handling missing values.*

####**Insights:**

* No found missing values

* No found duplicatyes values

* Dataset is relatively clean.

---
"""

#Check missing values
df.isnull().sum()

#Check duplicated values
print(df.duplicated().sum())

"""## 6.***EDA (Exploratory Data Analysis)***
________________________________________________________________________________________________________________________

*EDA is the process of visualizing and summarizing data to detect trends, patterns, and outliers.*

####**Insights:**

* Target Variable (Predictions):

  * Rich → 4589 (24%)

  * Poor → 17303 (75%) → dataset is imbalanced.

* Bar plots and count plots show distribution of categorical features.

---

###Check Categorical Target Column
---
"""

#Check Categorical Target Column
df['Predictions']

"""### Check value counts and percentage of target column
---
"""

#Check value counts of target
df.Predictions.value_counts()

# check Check value counts
df.Predictions.value_counts(normalize = True)

"""### Vertical Bar Plot
________________________________________________________________________________________________________________________
####*We use a bar plot when we want to compare counts or values across different categories in dataset.*
---
"""

#bar plot to check value counts of target
df.Predictions.value_counts().plot(kind = 'bar', color = ['skyblue', 'salmon'])
plt.title('Value counts of Income Column')
plt.xlabel('Income')
plt.xticks(rotation = 0)
plt.ylabel('Counts')
plt.show()

"""###Count Plot
---
#####*We use a bar plot when we want to compare counts or values across different categories in dataset.*
---
"""

# Check balance or imbalance count of target
sns.countplot(x='Predictions', data=df, palette='Set2')
plt.xlabel('Target')
plt.ylabel('Count')
plt.title('Value counts of target column')
plt.show()

"""###Pie Chart
---
*A pie chart displays the proportion of each category in a variable as slices of a circle, showing percentage distribution.*

---
"""

# Pie chart for target column
plt.figure(figsize=(6,6))
df['Predictions'].value_counts().plot.pie(autopct='%1.1f%%', startangle=90, colors=sns.color_palette("Set2"))
plt.title('Distribution of Target (Income)')
plt.ylabel('')  # removes y-label for cleaner look
plt.show()

"""##***7.Label Encoding of all object columns***
---
*Label Encoding converts categorical text values into numeric codes for ML models.*

####**Insights:**

* Columns like Gender → Male = 1, Female = 0.

* Target column Predictions → Poor vs Rich

* Makes dataset ML-ready.
---
"""

#label encoding of all columns
col = ['Working Section', 'education', 'Marriage Status', 'occupation', 'Relationship Status', 'Skin Color', 'Gender', 'Country', 'Predictions']
for col in col:
  le = LabelEncoder()
  df[col] = le.fit_transform(df[col])

df

"""##***8.Plots & Visualization***
---
*Visualizations help in interpreting patterns and feature relationships.*

---

### Pairplot
_________________________________________________________________________________________________________________________
#### *Pairplot scatterplot matix shows relationships between all numarical variables in dataset. It creates scatter plots for every possible pair of variables, and histograms on the diagonal to show distributions.*

---
"""

#pair plot all numarical features
sns.pairplot(df,hue = 'Predictions')

"""### Scatter Plot
_____________________________________________________________________________________________________________________
#### *Scatter plot shows reationship between two variables by plotting data points as dots on an X–Y plane. It helps to see trends, patterns, or clusters between those two variables.*
__________________________________________________________________________________________________________________________
"""

#Scatter plot between two variables
num_cols =['age', 'Financial Weight', 'education-num', 'Capital Gain', 'Capital Loss', 'hours-per-week', 'id']

#The loop is used to automatically generate scatter plots for each numerical column
for col in num_cols:
    sns.scatterplot(x=col, y='Financial Weight', data=df, hue='Predictions', palette='Set2')
    plt.title(f"Relationship between {col} and Predictions")
    plt.show()

"""###Count Plot
---
A count plot is a type of bar plot that shows the frequency (count) of each category in a categorical variable.

---
"""

#Count Plot of Categorical Columns
cat_cols = ['Working Section', 'education', 'Marriage Status', 'occupation', 'Relationship Status', 'Skin Color', 'Gender', 'Country', 'Predictions']

#The loop is used to automatically generate count plots for each categorical column
for col in cat_cols:
    plt.figure(figsize=(6,4))
    sns.countplot(x=col, data=df, palette="Set2")
    plt.title(f'Countplot of {col}')
    plt.show()

"""##***9.Relationship Between Numerical Features***
_________________________________________________________________________________________________________________________

###Correlation Heatmap
____________________________________________________________________________________________________________________
####*This heatmap shows the pairwise correlation between numerical features of the dataset.*
"""

#check Correlation
df.corr()

# ========= correlation matrix ==============
plt.figure(figsize = (15,15))
correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, annot = True, cmap = 'Blues')
plt.title("Correlation of Rich vs Poor dataset")
plt.show()

"""##***10.Feature & Target Separation***
---
*X contains independent variables (predictors).*

*y contains the dependent variable (Predictions).*

---
"""

#Traget & features sepreation
Y = df['Predictions']
X = df.drop(columns = ['Predictions', 'id'])

"""##***11.Over Sampling Using SMOTE***
---
SMOTE (Synthetic Minority Over-sampling Technique) balances class distribution by generating synthetic samples.

###**Insights:**

* Original: 75% Poor, 24% rich.

* After SMOTE: Balanced 50/50 dataset.

* Prevents models from being biased toward majority class.

---
"""

from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state = 45)
X_re, Y_re = smote.fit_resample(X,Y)

"""##***12.Train-Test Split***
__________________________________________________________________________________________________________________________
*The function train_test_split is used to divide a dataset into two parts: a training set and a testing set. The parameter test_size=0.20 means 20% of the data is used for testing, while 80% is used for training.
X_test.shape shows how many samples and features are present in the test dataset after splitting.*
"""

X_train,X_test,Y_train,Y_test = train_test_split(X_re,Y_re,test_size = 0.20, random_state= 45)

X_test.shape

print("X train shape:",X_train.shape)
print("Y train shape:",Y_train.shape)
print("X test shape:",X_test.shape)
print("Y test shape:",Y_test.shape)

X.columns

X_train.head()

"""##***13.Comparison Before And After SMOTE***
---
*Visual comparison to check how balancing affects dataset distribution.*

###**Insights:**

* Before SMOTE → Highly imbalanced.

* After SMOTE → Equal distribution of target labels.

---
"""

df[['Predictions']]

"""###Before Applying SMOTE
---
"""

sns.countplot(x = 'Predictions', data = df, palette = 'Set2')
plt.title("before applying SMOTE")
plt.show()

"""###After Applying SMOTE
---
"""

Y_re_df = pd.DataFrame(Y_re, columns = ['Predictions'])
sns.countplot(x = 'Predictions', data = Y_re_df, palette= 'Set2')
plt.title("after applying SMOTE")
plt.show()

"""##***14.Model Training***
---

*We apply multiple regression models:*

###**1.Logistic Regression**

*A linear model that estimates probabilities for binary classigication using the sigmoid function to classify outcomes.Soft max function for multi class classification using ONE VS REST*

###**2.Gaussian Naive Bayes:**

*Based on Bayes’ theorem, assumes feature independence and uses Gaussian distribution for probability.*

###**3.Decision Tree:**

*Splits data into branches based on feature values, leading to class labels at the leaf nodes.*

###**4.K-Nearest Neighbors (KNN):**

*Classifies a new data point based on the majority class of its k nearest neighbors using distance metrics.*

###**5.Support Vector Machine (SVM):**

*Finds the optimal hyperplane that separates classes with maximum margin close to hyperplane are support vectors.*
###**5.Random Forest:**

*An ensemble of multiple decision trees where each tree votes, and the majority determines the final prediction.*

---
"""

# ====================== MDEOL TRAING =============================
# ===================== Models Dictionary =========================
models = {
    "                   Logistic Regression": LogisticRegression(),
    "                  Gaussian Naive Bayes": GaussianNB(),
    "                         Decision Tree": DecisionTreeClassifier(),
    "                   K-Nearest Neighbors": KNeighborsClassifier(),
    "                Support Vector Machine": SVC(),
    "                          RandomForest": RandomForestClassifier()
}

#Dictionary to store results
results = {}

#Loop through models
for name , model in models.items():
  model.fit(X_train, Y_train)
  print(name + "Trained.")

"""##***15.Accuracy***
---
*Accuracy = % of correct predictions made by the model.*

###**Insights:**

* Logistic Regression → 69.10%

* Naive Bayes → 62.77%

* Decision Tree → 84.93%

* KNN → 70.89%

* SVM → 58.87%

* Random Forest → 88.92% (best)

---
"""

# Loop through all models stored in the dictionary 'models'
for name, model in models.items():

  #calculate accuracy of each model
  accuracy = model.score(X_test, Y_test)*100
  #print accuracy
  print(name + ":{:.2f}%".format(accuracy))
  # Store the accuracy results in the dictionary
  results[name] = accuracy

"""##***16.Model Accuracy Comparision With Bar Chart***
---
*Comparing multiple models shows which performs best on the dataset.
Convert results dictionary into DataFrame (results_df).
Print table of models with their accuracies.
Plot horizontal bar chart for visual comparison.
Add labels, colors, and title for clarity.*

###**Insights:**

* Random Forest outperformed all others.

* Decision Tree also performed well.

* Logistic Regression gave baseline performance.

---
"""

#Convert to data frame
results_df = pd.DataFrame(list(results.items()), columns = ["Model", "Accuracy"])

#Display accuracy
print("\n ======= Model Comparision: =======")
print(results_df)

#Bar Chart for comparision
plt.figure(figsize = (6,6))
colors = sns.color_palette('pastel')
plt.barh(results_df["Model"], results_df['Accuracy'], color = colors)
plt.xlabel("Accuracy %")
plt.title("Model Accuracy Comparison")
plt.show()

"""##***17.Model Accuracy Comparison using Pie Chart***
---
*pie chart shows the accuracy proportion of each model. Bigger slices represent models with higher accuracy, making it easy to compare performance at a glance.*

---
"""

# =============== PIE CHART ===================
# Convert to data frame
results_df = pd.DataFrame(list(results.items()), columns = ["Model", "Accuracy"])

# Display accuracy table
print("\n ======= Model Comparision: =======")
print(results_df)

# Pie Chart for comparison
plt.figure(figsize=(7,7))
colors = sns.color_palette('pastel')
plt.pie(
    results_df['Accuracy'],
    labels=results_df['Model'],
    autopct='%1.1f%%',
    startangle=140,
    colors=colors
)
plt.title("Model Accuracy Comparison (Pie Chart)")
plt.show()

"""##***18.Confusion Matrix & Heatmap***
---
*Confusion Matrix shows counts of True Positives, False Positives, True Negatives, False Negatives*.

###**Insights:**

* Random Forest had balanced precision & recall.

* SVM predicted majority class correctly but failed minority class.

---
"""

#=============== CONFUSION MATRIX =======================
# Loop through all models stored in the dictionary 'models'.
for name, model in models.items():
  Y_pred = model.predict(X_test)
  print("Confusion Matrix")
  cm = confusion_matrix(Y_test, Y_pred)
  print(cm)

  # =============== HEATMAP ======================
  plt.figure (figsize = (6, 4))
  sns.heatmap(cm, annot = True, fmt = 'd', cmap = "Blues", cbar = False, annot_kws = {'size': 14})
  plt.xlabel('Predicted Values', fontsize = 14)
  plt.ylabel('Actual Values', fontsize = 14)
  plt.title("Confusion Matrix" + name, fontsize = 16)
  plt.show()

"""##***19.Classification Report***
---
*Report with precision, recall, and F1-score for each class.*

###**Insights:**

* Random Forest: Best overall (precision & recall ≈ 0.89).

* Decision Tree: Balanced performance (≈0.85).

* Naive Bayes & SVM struggled with minority class detection.

---
"""

#=============== CLASSIFICATION REPORT =======================
# Loop through all models stored in the dictionary 'models'
for name , model in models.items():
  print(f"\n ========= {name} =========")
  Y_pred = model.predict(X_test)
  print(classification_report(Y_test, Y_pred))

"""##***20.Summary***
__________________________________________________________________________________________________________________________
### EDA (Exploratory data analysis)
* Loading & reading dataset (Pandas)
  *  import pandas as pd
* Exploration dataset
  * df.head -> first 5 rows
  *  df.tail -> last 5 rows
  *  df.info -> total rows & columns, null values, data types
  *  df.describe -> summary for numarical columns
  *  df.value_counts-> summary for object columns
  *  df.columns -> column names
  *  df.shape -> rows & columns
  *  df.dtypes -> datatypes
* Data cleaning
   * Dataset is already clean
###Machine Learning types
  * Three Tpyes:
    * supervised learning
    * unsupervised learning
    * reinforcement learning
  * Supervied learning ( labeled data)
    * Classification -> categorical target column (Predictions)
      * visualization ->  bar plot, count plot , pie plot
###Visualization:
  * Basic Matplotlib
    * Bar plot -> Categorical target column
    * Count plot -> Check balance & imbalance target
    * Pie plot -> Check percentage distribution for categorical target
  * Advance Seaborn
    * Pair plot -> Relationship between all numerical variables
    * Count Plot -> Check Value counts of all categorical variables
    * Scatter plot -> Relation between two variables
    * Correlation Heatmap -> Correlation between numetical features.
### Feature Engineering / Encoding
  * Used LabelEncoder on categorical columns → converted to numeric.
  * Target column (Predictions) encoded as 0/1.
###Train-Test Split
   * train_test_split(test_size=0.20, random_state=45) → 80/20 split.
   * Shapes:
      *  X train shape: (27684, 14)
      * Y train shape: (27684,)
      * X test shape: (6922, 14)
      * Y test shape: (6922,)

###Over Sampling Using SMOTE

* Original dataset → imbalanced (75% Poor, 24% Rich).

* Applied SMOTE to generate synthetic samples for minority class.

* After balancing → 50/50 distribution between both classes.

###Model Training

**Applied multiple classification models:**

* Logistic Regression

* Gaussian Naive Bayes

* Decision Tree

* K-Nearest Neighbors (KNN)

* Support Vector Machine (SVM)

* Random Forest

###Model Evaluation (Accuracy %)

* Logistic Regression → 69.10%

* Naive Bayes → 62.77%

* Decision Tree → 84.93%

* KNN → 70.89%

* SVM → 58.87%

* Random Forest → 88.92% (best)

###Confusion Matrix & Classification Report

* Confusion Matrix → showed true vs predicted outcomes.

* Classification Report → precision, recall, and F1-score.

* Random Forest gave the best balanced performance.

* SVM struggled with minority class.

* Decision Tree had strong overall scores.

###Key Insights from EDA & Modeling

* Factors like education level, working hours, occupation, capital-gain strongly influence income.

* Dataset needed balancing due to class imbalance.

* Random Forest model captured relationships best and achieved the highest accuracy.
  ___________________________________________________________________________________________________________

##***21.Best Model : Random Forest***
---
*Among all models, **Random Forest** performed the best with an accuracy of 88.81%. It is an **ensemble learning algorithm** that combines multiple decision trees to improve prediction performance and reduce overfitting. This makes it more reliable and accurate compared to individual models.*

---

##***22.Final Calculation / Result***
---

*After evaluating all models, the results show that Random Forest achieved the highest accuracy (88.81%), followed by Decision Tree (84.93%). Other models like Logistic Regression (69.09%), KNN (70.84%), Naive Bayes (62.77%), and SVM (58.87%) performed comparatively lower. Hence, Random Forest is the best performing model for this dataset.*

---
"""